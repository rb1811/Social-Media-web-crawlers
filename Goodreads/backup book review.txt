soup.find_all('div',attrs={'class':'friendReviews elementListBrown'})


import requests,urllib2,time
from bs4 import BeautifulSoup
from selenium import webdriver
import json
import time
first_part='https://www.goodreads.com'
second_part =""
third_part = 'authenticity_token=kgTHED0Aa4Ah1aP93G37AqfQmz%2F5dbCmmPt3IScZ%2B%2BqANzOJw1yw2HGjSepcQHPkjEd0mNQZDjbu81Vsgp%2B1SQ%3D%3D&amp;hide_last_page=true&amp;page='
fourth_part=""
fifth_part= '&authenticity_token=nXQXzqHMY4usRlL%2FBOkVL0DRh5Kfnd35bFcgZcpdk5CPR%2BNXX5C40%2FwwuOiExJ3Ja0ZoNbLxY2kaXwIob9vdMw%3D%3D'
user_reviews = []
header ={
		  	"Accept":"text/javascript, text/html, application/xml, text/xml, */*",
			"Accept-Encoding":"gzip, deflate, sdch, br",
			"Accept-Language":"en-US,en;q=0.8",
			"Connection":"keep-alive",
			"Cookie":"csid=BAhJIhg4NzYtNDc5MTMwNy03MzE3MzMwBjoGRVQ%3D--9015ae6c6997fb0c3dbe115af175567dd8860898; __qca=P0-1986983302-1491770731021; fbm_2415071772=base_domain=.goodreads.com; u=An-NUvgkfF3Ew623eNkUqV9N_hk1KmWjkoJIPgAmSYdHBsTB; p=J4omKhY0N867ks24pDMwMK6ISLLc4pLinD9fYL8eVt0tUqKv; fbl=true; locale=en; csm-sid=293-1514290-6245195; __utma=250562704.2145582756.1491770731.1492210866.1492220542.3; __utmc=250562704; __utmz=250562704.1492210866.2.2.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); fbsr_2415071772=Ca5ZyVqCxOC62lWyWIe-GtOEB_Ta0_soRT1k2M7bfvY.eyJhbGdvcml0aG0iOiJITUFDLVNIQTI1NiIsImNvZGUiOiJBUUJjMVR1ZnlJbFFkV2tBMDBxckhRVnN1Umx3aTFkT2IwdmotUXJ0RGp0SWlzWU0tSTYtYndISHdJYUIzaFpMc3lEUXA0Umo5ZFpfMkxVNnlhOWxQazUwLXJtZS1DMUxRcXpIZ05CWjJfVnNKbFhfNnRKUlNuMGpZYlhmWmItNTNJbzAxaDNfVXBtX1ZhNjhzakg2aDBxRmZwMWF0ZHliUXBSLTZHaGlCLUN6MW91aWcyd29MUmUyRXYwSnhGbmFBQVl3Ml9EZ3BSQlNGY2RSbXhadTR6SXk4aFl3WThBODB2S2JTV1FxUW02R1BQMGJEcFo0WFpTYnl6V3VKaGJySVZjYmJ1VklxZFlUMXZMZTlycWdLZ3NkSDEwWWFUMU9kb1V2T1Q5TjFJM1l3TlpOdmJ6MklSWW52amRfU2FlYXJCOS11OHV3Rm5DUDZSdjgwNHA3WGR1RFQtSVFPbDBmQW95RVEtRzhzQzlnS2V6emlDMC1wVWhhY2d2bUtReWNCSDAiLCJpc3N1ZWRfYXQiOjE0OTIyMjMxMTIsInVzZXJfaWQiOiIxMDAwMDM5NjAxMzMwMjQifQ; _session_id2=a35e635abc06c8a3c507d1359bcc8bb6",
			"DNT":"1",
			"Host":"www.goodreads.com",
			"Referer":"https://www.goodreads.com/book/show/14313.Anna_Karenina_by_Leo_Tolstoy_Fiction_Classics_Literary?from_search=true",
			"Related-Request-Id":"7R98YGBNFAAE04228W0Z",
			"User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36",
			"X-CSRF-Token":"Sn0ptQxolaW59MyFvmSKLT8L2f7RkGzBYzw6Nm2THXBYTt0s8jRO/emCJpI+SQLLFJw2Wfz80lEVNBh7yBVT0w==",
			"X-Prototype-Version":"1.7",
			"X-Requested-With":"XMLHttpRequest",

	  	}
# urlStr = []
# g = open('goodreads_book.json','r')
# book_data =  json.loads(g.read())
# for i in range(len(book_data)):
# 	key  = book_data[i].keys()[0]
# 	for link in book_data[i][key]['book_urls']:
# 		urlStr.append('https://www.goodreads.com'+link)
# g.close()
urlStr =['https://www.goodreads.com/book/show/20739628-jane-eyre?from_search=true', ]
print "No of books to be scraped:",len(urlStr)
# 'https://www.goodreads.com/book/show/27862725-primeval-origins'

def getDriver():
	print "Driver function called"
	driver = webdriver.PhantomJS(executable_path='/usr/local/share/phantomjs/bin/phantomjs')
	urlStr = "https://www.goodreads.com/user/sign_in"

	try:
		driver.get(urlStr)
		# time.sleep(0.5)
		driver.find_element_by_id("user_email").send_keys("kai.shu00@gmail.com")
		driver.find_element_by_id("user_password").send_keys("19900327sk")
		login = driver.find_element_by_xpath("/html/body/div[1]/div[1]/div[2]/div/div/div/div[2]/form/fieldset/div[5]/input")
		login.click()
	except Exception as e:
		print e.message
	return driver

# driver = getDriver()
for new_book in urlStr:
	print "This is the current book being scrapped", new_book	
	# driver.get(new_book)
	# response = driver.page_source
	response =  requests.get(new_book)
	soup = BeautifulSoup(response.content, "lxml")
	user_url, user_names, ratings,likes_url = [], [],[],[]
	previous_page = 1
	for link in soup.find_all('a',attrs= {"class":"user"}):
			user_url.append(link['href'])
	for url in user_url:
			user_names.append(' '.join(url[url.find('-')+1:].split('-')))
	# print user_names

	all_ratings = soup.find_all('span','staticStars')[1:]
	for ele in all_ratings:
		current_rate = ele.find('span',attrs={"class":"staticStar"}).string
		if 'it was amazing' == current_rate:
			ratings.append(5)
		elif 'really liked it' == current_rate:
			ratings.append(4)
		elif 'liked it' == current_rate:
			ratings.append(3)
		elif 'it was ok' == current_rate:
			ratings.append(2)
		elif 'did not like it' == current_rate:
			ratings.append(1)
		else:
			ratings.append(None)
	
	print ratings
	temp_likes_url = soup.find_all('div',attrs={"class":"left bodycol"})
	for ele in temp_likes_url:
		likecontainer  = ele.find('span',attrs={"class":"likeItContainer"})
		if 'like_count_review' in str(likecontainer):
			likes_url.append(likecontainer.find('a')['href'])
		else:
			likes_url.append(None)
	print "#################"
	print "usernames: user_url: ratings:  likes_url ",len(user_names), len(user_url), len(ratings), len(likes_url)
	print "#################"
	while 1:
		print  previous_page 
		print "@@@@@@@@@@@@@"
		next_page = soup.find('a',attrs={"class":"next_page"})['onclick']
		if next_page:
			temp_second_part = next_page[next_page.find("'")+1:next_page.find(',')]
			fourth_part=temp_second_part[temp_second_part.rfind('=')+1:][:-1]
			print "next page ",fourth_part
			second_part = temp_second_part[:temp_second_part.find('?')+1]
			ajax_url =   first_part+second_part+third_part+str(fourth_part)+fifth_part 
			print ajax_url
			print "############"
			if previous_page != int(fourth_part):
				previous_page  = int(fourth_part) 
				response = requests.get(ajax_url,headers=header)
				time.sleep(0.5)
				data = response.content.decode('unicode-escape')
				soup = BeautifulSoup(data, "lxml")
				for link in soup.find_all('a',attrs= {"class":"user"}):
					if link['href'] not in user_url:
						user_url.append(link['href'])
						user_names.append(' '.join(link['href'][link['href'].find('-')+1:].split('-')))
						print link['href']
						print len(user_url)

				all_ratings = soup.find_all('span','staticStars')
				print "len:",len(all_ratings)
				for ele in all_ratings:
					current_rate = ele.find('span',attrs={"class":"staticStar"}).string
					if 'it was amazing' == current_rate:
						# print 'it was amazing'
						ratings.append(5)
					elif 'really liked it' == current_rate:
						# print 'really liked it'
						ratings.append(4)
					elif 'liked it' == current_rate:
						# print 'liked it'
						ratings.append(3)
					elif 'it was ok' == current_rate:
						# print 'it was ok'
						ratings.append(2)
					elif 'did not like it' == current_rate:
						# print 'did not like it'
						ratings.append(1)
					else:
						# print "Noen"
						ratings.append(0)
				# print ratings
				temp_likes_url = soup.find_all('div',attrs={"class":"left bodycol"})
				for ele in temp_likes_url:
					likecontainer  = ele.find('span',attrs={"class":"likeItContainer"})
					if 'like_count_review' in str(likecontainer):
						likes_url.append(likecontainer.find('a')['href'])
					else:
						likes_url.append(0)

				print "&&&&&&&&&&&&&&&&&&&"
				print "usernames: user_url: ratings:  likes_url ",len(user_names), len(user_url), len(ratings), len(likes_url)
				print "&&&&&&&&&&&&&&&&&&&"
	
			else:
				break
		else:
			break
	temp_dict={}
	user_reviews.append(
			{
				new_book:[
				{"user_url":user_url},
				{"ratings" : ratings},
				{"user_names": user_names},
				{"likes_url": likes_url}

						]
			} 

		)
# print user_reviews
f = open('user_reviews.json', 'w')
for ele in user_reviews:
	json.dump(ele,f)
	f.write(','+'\n')
f.close()		